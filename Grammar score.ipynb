{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b15ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "# Force pure-Python protocol buffers to avoid TensorFlow proto conflicts.\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e560bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torch==2.7.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torchaudio) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchaudio) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchaudio) (3.3.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchaudio) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchaudio) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchaudio) (2021.10.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.0->torchaudio) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.0->torchaudio) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cb5562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: librosa\n",
      "Version: 0.11.0\n",
      "Summary: Python module for audio and music processing\n",
      "Home-page: https://librosa.org\n",
      "Author: Brian McFee, librosa development team\n",
      "Author-email: brian.mcfee@nyu.edu\n",
      "License: ISC\n",
      "Location: c:\\users\\shilpi singh\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: scipy, lazy_loader, scikit-learn, soxr, typing_extensions, msgpack, numba, decorator, audioread, soundfile, joblib, numpy, pooch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e34502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (1.22.4)\n",
      "Collecting audioread>=2.1.9\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (4.13.2)\n",
      "Collecting lazy_loader>=0.1\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting pooch>=1.1\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (0.54.1)\n",
      "Collecting soxr>=0.3.2\n",
      "  Using cached soxr-0.5.0.post1-cp39-cp39-win_amd64.whl (167 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (5.1.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from lazy_loader>=0.1->librosa) (21.0)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.37.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (58.0.4)\n",
      "Collecting numba>=0.51.0\n",
      "  Using cached numba-0.60.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Using cached llvmlite-0.43.0-cp39-cp39-win_amd64.whl (28.1 MB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from packaging->lazy_loader>=0.1->librosa) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.20)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Installing collected packages: llvmlite, soxr, soundfile, pooch, numba, lazy-loader, audioread, librosa\n",
      "Successfully installed audioread-3.0.1 lazy-loader-0.4 librosa-0.11.0 llvmlite-0.43.0 numba-0.60.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477c5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from xgboost) (1.22.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6721a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (8.4.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.3.1)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.22.4)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shilpi singh\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Installing collected packages: fsspec, huggingface-hub, tokenizers, safetensors, transformers, sentence-transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.10.1\n",
      "    Uninstalling fsspec-2021.10.1:\n",
      "      Successfully uninstalled fsspec-2021.10.1\n",
      "Successfully installed fsspec-2025.3.2 huggingface-hub-0.30.2 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c8c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\r\n",
      "built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\r\n",
      "configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\r\n",
      "libavutil      59. 39.100 / 59. 39.100\r\n",
      "libavcodec     61. 19.101 / 61. 19.101\r\n",
      "libavformat    61.  7.100 / 61.  7.100\r\n",
      "libavdevice    61.  3.100 / 61.  3.100\r\n",
      "libavfilter    10.  4.100 / 10.  4.100\r\n",
      "libswscale      8.  3.100 /  8.  3.100\r\n",
      "libswresample   5.  3.100 /  5.  3.100\r\n",
      "libpostproc    58.  3.100 / 58.  3.100\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print(subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True).stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1d7f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "import librosa\n",
    "import whisper\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Regressors (choose one or experiment with several)\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# For text embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74f81258",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"train_csv\": \"C:/Users/Shilpi Singh/shl-intern-hiring-assessment/Dataset/train.csv\",\n",
    "    \"test_csv\": \"C:/Users/Shilpi Singh/shl-intern-hiring-assessment/Dataset/test.csv\",\n",
    "    \"audios_train\": \"C:/Users/Shilpi Singh/shl-intern-hiring-assessment/Dataset/audios/train\",\n",
    "    \"audios_test\": \"C:/Users/Shilpi Singh/shl-intern-hiring-assessment/Dataset/audios/test\",\n",
    "    \"sample_submission\": \"C:/Users/Shilpi Singh/shl-intern-hiring-assessment/Dataset/sample_submission.csv\",\n",
    "    \"output_submission\": \"C:/Users/Shilpi Singh/shl-intern-hiring-assessment/Dataset/submission.csv\",\n",
    "    # Audio processing\n",
    "    \"target_sample_rate\": 16000,\n",
    "    \"max_audio_length\": 10,  # seconds\n",
    "}\n",
    "CONFIG[\"max_audio_length_samples\"] = CONFIG[\"target_sample_rate\"] * CONFIG[\"max_audio_length\"]\n",
    "\n",
    "# Utility download function (for Kaggle output)\n",
    "def download_file(path, download_file_name):\n",
    "    zip_name = f\"{download_file_name}.zip\"\n",
    "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
    "        zipf.write(path, os.path.basename(path))\n",
    "    print(f\"Zipped submission saved as: {os.path.abspath(zip_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee159ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_acoustic_features(audio_path, whisper_model):\n",
    "    \"\"\"\n",
    "    Loads and processes audio for Whisper.\n",
    "    Uses Whisper's log-Mel spectrogram and encoder.\n",
    "    Returns a deep acoustic feature vector by mean-pooling.\n",
    "    \"\"\"\n",
    "    # Load and pad audio via Whisper utilities\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Get encoder output; shape: [batch, frames, hidden_size]\n",
    "        encoded = whisper_model.encoder(mel.unsqueeze(0))\n",
    "    # Mean pool over time dimension to get a single feature vector\n",
    "    acoustic_feature = encoded.squeeze(0).mean(dim=0).cpu().numpy()\n",
    "    return acoustic_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133d6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_handcrafted_features(audio_path, sr=16000):\n",
    "    \"\"\"\n",
    "    Loads audio with librosa and computes:\n",
    "      - MFCC means and standard deviations (n_mfcc=13)\n",
    "      - Zero crossing rate (mean and std)\n",
    "      - RMS energy (mean and std)\n",
    "    Returns a vector of handcrafted acoustic features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_std = np.std(mfcc, axis=1)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        handcrafted = np.concatenate([mfcc_mean, mfcc_std, [np.mean(zcr), np.std(zcr)], [np.mean(rms), np.std(rms)]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting handcrafted features from {audio_path}: {e}\")\n",
    "        # If error, return zeros (length: 13+13+2+2 = 30)\n",
    "        handcrafted = np.zeros(30)\n",
    "    return handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47314459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(audio_path, whisper_model, text_encoder):\n",
    "    \"\"\"\n",
    "    Uses Whisper to transcribe audio and a SentenceTransformer to encode the transcript.\n",
    "    Returns a text embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = whisper_model.transcribe(audio_path, fp16=False)\n",
    "        transcript = result['text']\n",
    "        text_embed = text_encoder.encode(transcript)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing or encoding text from {audio_path}: {e}\")\n",
    "        # If error, return zeros (assume text embeddings are length 768)\n",
    "        text_embed = np.zeros(768)\n",
    "    return text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0bbcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_duration(audio_path, sr=16000):\n",
    "    try:\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        duration = len(y) / sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing duration for {audio_path}: {e}\")\n",
    "        duration = 0.0\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838771d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hybrid_features(df, audio_folder, whisper_model, text_encoder):\n",
    "    \"\"\"\n",
    "    For each audio file:\n",
    "      - Extract deep acoustic features (from Whisper encoder)\n",
    "      - Extract handcrafted acoustic features (MFCCs, ZCR, RMS)\n",
    "      - Extract text (linguistic) features (Whisper transcription + SentenceTransformer)\n",
    "      - Compute duration\n",
    "    Then concatenates all feature vectors into one combined feature vector.\n",
    "    \"\"\"\n",
    "    combined_features = []\n",
    "    for file in tqdm(df['filename'], desc=\"Extracting hybrid features\"):\n",
    "        file_path = os.path.join(audio_folder, file)\n",
    "        # Deep acoustic representation (e.g., 512-dim or similar)\n",
    "        acoustic_feat = extract_acoustic_features(file_path, whisper_model)\n",
    "        # Handcrafted features (30-dimensional, as defined above)\n",
    "        handcrafted_feat = extract_handcrafted_features(file_path, sr=CONFIG[\"target_sample_rate\"])\n",
    "        # Text features (e.g., 768-dim from SentenceTransformer)\n",
    "        text_feat = extract_text_features(file_path, whisper_model, text_encoder)\n",
    "        # Duration as scalar\n",
    "        duration = compute_duration(file_path, sr=CONFIG[\"target_sample_rate\"])\n",
    "        # Optionally, you can normalize duration (e.g., divide by 60) later during training\n",
    "        # Concatenate all features into one vector\n",
    "        features = np.concatenate([acoustic_feat, handcrafted_feat, text_feat, [duration]])\n",
    "        combined_features.append(features)\n",
    "    combined_features = np.array(combined_features)\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a908682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (for both acoustic and transcription) ...\n",
      "Loading SentenceTransformer model for text embeddings ...\n",
      "Extracting hybrid features for training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hybrid features: 100%|██████████████████████████████████████████████████| 444/444 [2:23:59<00:00, 19.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load CSV files\n",
    "train_df = pd.read_csv(CONFIG[\"train_csv\"])\n",
    "test_df = pd.read_csv(CONFIG[\"test_csv\"])\n",
    "\n",
    "# Create full paths for audio files\n",
    "train_df['file_path'] = train_df['filename'].apply(lambda x: os.path.join(CONFIG[\"audios_train\"], x))\n",
    "test_df['file_path'] = test_df['filename'].apply(lambda x: os.path.join(CONFIG[\"audios_test\"], x))\n",
    "# Load models for feature extraction\n",
    "print(\"Loading Whisper model (for both acoustic and transcription) ...\")\n",
    "whisper_model = whisper.load_model(\"base\").to(device)\n",
    "print(\"Loading SentenceTransformer model for text embeddings ...\")\n",
    "text_encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Extract hybrid features for training\n",
    "print(\"Extracting hybrid features for training ...\")\n",
    "X = extract_hybrid_features(train_df, CONFIG[\"audios_train\"], whisper_model, text_encoder)\n",
    "y = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c49fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and validation sets ...\n",
      "Training XGBoost Regressor on hybrid features ...\n",
      "Validation RMSE (XGBoost): 0.8333\n",
      "Training MLP Regressor on hybrid features ...\n",
      "Validation RMSE (MLP Neural Net): 1.0154\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data into training and validation sets ...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Choose a regressor; for example, XGBoost\n",
    "print(\"Training XGBoost Regressor on hybrid features ...\")\n",
    "model_xgb = XGBRegressor(n_estimators=400, learning_rate=0.009, max_depth=6, random_state=42)\n",
    "model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "val_preds = model_xgb.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "print(f\"Validation RMSE (XGBoost): {rmse:.4f}\")\n",
    "\n",
    "# Alternatively, you can try an MLP:\n",
    "print(\"Training MLP Regressor on hybrid features ...\")\n",
    "model_mlp = MLPRegressor(hidden_layer_sizes=(512,464,256), activation='tanh',\n",
    "                         solver='sgd', max_iteprint(\"Splitting data into training and validation sets ...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Choose a regressor; for example, XGBoost\n",
    "print(\"Training XGBoost Regressor on hybrid features ...\")\n",
    "model_xgb = XGBRegressor(n_estimators=400, learning_rate=0.009, max_depth=6, random_state=42)\n",
    "model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "val_preds = model_xgb.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "print(f\"Validation RMSE (XGBoost): {rmse:.4f}\")\n",
    "\n",
    "# Alternatively, you can try an MLP:\n",
    "print(\"Training MLP Regressor on hybrid features ...\")\n",
    "model_mlp = MLPRegressor(hidden_layer_sizes=(512,464,256), activation='tanh',\n",
    "                         solver='sgd', max_iter=500000, random_state=42)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "val_preds_mlp = model_mlp.predict(X_val)\n",
    "rmse_mlp = mean_squared_error(y_val, val_preds_mlp, squared=False)\n",
    "print(f\"Validation RMSE (MLP Neural Net): {rmse_mlp:.4f}\")r=500000, random_state=42)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "val_preds_mlp = model_mlp.predict(X_val)\n",
    "rmse_mlp = mean_squared_error(y_val, val_preds_mlp, squared=False)\n",
    "print(f\"Validation RMSE (MLP Neural Net): {rmse_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e37e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting hybrid features for test set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hybrid features: 100%|████████████████████████████████████████████████████| 204/204 [53:23<00:00, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test set with XGBoost ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting hybrid features for test set ...\")\n",
    "X_test = extract_hybrid_features(test_df, CONFIG[\"audios_test\"], whisper_model, text_encoder)\n",
    "print(\"Predicting on test set with XGBoost ...\")\n",
    "test_preds = model_xgb.predict(X_test)\n",
    "test_preds = np.clip(test_preds, 0, 5)  # Clip to valid range if necessary\n",
    "# (Optional) Smoothing: combine with overall training mean, e.g.:\n",
    "test_preds = 0.9 * test_preds + 0.1 * y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7135e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!\n",
      "Submission file path: C:\\Users\\Shilpi Singh\\shl-intern-hiring-assessment\\Dataset\\submission.csv\n",
      "Zipped submission saved as: C:\\Users\\Shilpi Singh\\out.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "# Create and save submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"filename\": test_df[\"filename\"],\n",
    "    \"label\": test_preds\n",
    "})\n",
    "submission_df.to_csv(CONFIG[\"output_submission\"], index=False)\n",
    "print(\"Submission file saved!\")\n",
    "print(\"Submission file path:\", os.path.abspath(CONFIG[\"output_submission\"]))\n",
    "download_file(CONFIG[\"output_submission\"], \"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd85f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d4ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
